<div align="center">

<h1>Control Color: Multimodal Diffusion-based Interactive Image Colorization</h1>

<div>
    <a href='https://zhexinliang.github.io/' target='_blank'>Zhexin Liang</a>&emsp;
    <a href='' target='_blank'>Zhaochen Li</a>&emsp;
    <a href='https://shangchenzhou.com/' target='_blank'>Shangchen Zhou</a>&emsp;
    <a href='https://li-chongyi.github.io/' target='_blank'>Chongyi Li</a>&emsp;
    <a href='https://www.mmlab-ntu.com/person/ccloy/' target='_blank'>Chen Change Loy</a>
</div>
<div>
    S-Lab, Nanyang Technological University&emsp; 
</div>

<!-- <div>
    :star: <strong>Accepted to ICCV 2023, Oral</strong>
</div> -->
<div>
    <h4 align="center">
        <a href="https://zhexinliang.github.io/Control_Color/" target='_blank'>[Project Page]</a> •
<!--         <a href="https://arxiv.org/abs/2303.17569" target='_blank'>[arXiv]</a> • -->
        <a href="https://youtu.be/tSCwA-srl8Q" target='_blank'>[Demo Video]</a>
    </h4>
</div>


<strong>Control Color (CtrlColor) achieves highly controllable multimodal image colorization based on stable diffusion model.</strong>
<!-- 
<table>
<tr>
    <td><img src="assets/0032_rgb.gif" width="100%"/></td>
    <td><img src="assets/0032_geo.gif" width="100%"/></td>
    <td><img src="assets/0067_rgb.gif" width="100%"/></td>
    <td><img src="assets/0067_geo.gif" width="100%"/></td>
    <td><img src="assets/0021_rgb_dancing.gif" width="98%"/></td>
    <td><img src="assets/0001_rgb_interpolation.gif" width="88%"/></td>
</tr>
<tr>
    <td align='center' width='14%'>Sample 1 RGB</td>
    <td align='center' width='14%'>Sample 1 Geo</td>
    <td align='center' width='14%'>Sample 2 RGB</td>
    <td align='center' width='14%'>Sample 2 Geo</td>
    <td align='center' width='19%'>Novel Pose Generation</td>
    <td align='center' width='19%'>Latent Space Interpolation</td>
</tr>
</table> -->

:open_book: For more visual results and applications of CtrlColor, go checkout our <a href="https://zhexinliang.github.io/Control_Color/" target="_blank">project page</a>.

---


</div>

## :mega: About code publication
We will release the code in 2024.

<!--
## :love_you_gesture: Citation
If you find our work useful for your research, please consider citing the paper:
```
@inproceedings{liang2023iterative,
  title={Iterative prompt learning for unsupervised backlit image enhancement},
  author={Liang, Zhexin and Li, Chongyi and Zhou, Shangchen and Feng, Ruicheng and Loy, Chen Change},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={8094--8103},
  year={2023}
}
```
-->
### Contact
If you have any questions, please feel free to reach out at `zhexinliang@gmail.com`. 

<!-- ## :newspaper_roll: License

Distributed under the S-Lab License. See `LICENSE` for more information.

## :raised_hands: Acknowledgements

This study is supported by NTU NAP, MOE AcRF Tier 2 (T2EP20221-0033), and under the RIE2020 Industry Alignment Fund – Industry Collaboration Projects (IAF-ICP) Funding Initiative, as well as cash and in-kind contribution from the industry partner(s).

This project is built on source codes shared by [Style -->

